{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "phantom-uganda",
   "metadata": {},
   "source": [
    "# Activity 1.1 : Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-highway",
   "metadata": {},
   "source": [
    "#### Objective(s):\n",
    "\n",
    "This activity aims to demonstrate the concepts of neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intermediate-cleveland",
   "metadata": {},
   "source": [
    "#### Intended Learning Outcomes (ILOs):\n",
    "* Demonstrate how to use activation function in neural networks\n",
    "* Demonstrate how to apply feedforward and backpropagation in neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-coupon",
   "metadata": {},
   "source": [
    "#### Resources:\n",
    "* Jupyter Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-marriage",
   "metadata": {},
   "source": [
    "#### Procedure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-fleece",
   "metadata": {},
   "source": [
    "Import the libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "horizontal-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-thriller",
   "metadata": {},
   "source": [
    "Define and plot an activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-fluid",
   "metadata": {},
   "source": [
    "### Sigmoid function:\n",
    "\n",
    "$$\n",
    "\\sigma = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "$\\sigma$ ranges from (0, 1). When the input $x$ is negative, $\\sigma$ is close to 0. When $x$ is positive, $\\sigma$ is close to 1. At $x=0$, $\\sigma=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "extreme-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a sigmoid function\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "amazing-tongue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sigmoid function\n",
    "vals = np.linspace(-10, 10, num=100, dtype=np.float32)\n",
    "activation = sigmoid(vals)\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "fig.suptitle('Sigmoid function')\n",
    "plt.plot(vals, activation)\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.axvline(x=0, color='k')\n",
    "plt.yticks()\n",
    "plt.ylim([-0.5, 1.5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-grace",
   "metadata": {},
   "source": [
    "Choose any activation function and create a method to define that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "domestic-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here\n",
    "def relu(x):\n",
    "  \"\"\"ReLU activation function.\"\"\"\n",
    "  return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-longer",
   "metadata": {},
   "source": [
    "Plot the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "absolute-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your code here\n",
    "vals = np.linspace(-10, 10, num=100, dtype=np.float32)\n",
    "activation = relu(vals)\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "fig.suptitle('ReLU function')\n",
    "plt.plot(vals, activation)\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.axvline(x=0, color='k')\n",
    "plt.yticks()\n",
    "plt.ylim([-0.5, 1.5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-sustainability",
   "metadata": {},
   "source": [
    "### Neurons as boolean logic gates\n",
    "\n",
    "\n",
    "\n",
    "### OR Gate\n",
    "\n",
    "<table>\n",
    "\n",
    "<tr>\n",
    "<th colspan=\"3\">OR gate truth table</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th colspan=\"2\">Input</th>\n",
    "<th>Output</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "A neuron that uses the sigmoid activation function outputs a value between (0, 1). This naturally leads us to think about boolean values.\n",
    "\n",
    "\n",
    "\n",
    "By limiting the inputs of $x_1$ and $x_2$ to be in $\\left\\{0, 1\\right\\}$, we can simulate the effect of logic gates with our neuron. The goal is to find the weights , such that it returns an output close to 0 or 1 depending on the inputs.\n",
    "\n",
    "What numbers for the weights would we need to fill in for this gate to output OR logic? Observe from the plot above that $\\sigma(z)$ is close to 0 when $z$ is largely negative (around -10 or less), and is close to 1 when $z$ is largely positive (around +10 or greater).\n",
    "\n",
    "$$\n",
    "z = w_1 x_1 + w_2 x_2 + b\n",
    "$$\n",
    "\n",
    "Let's think this through:\n",
    "\n",
    "* When $x_1$ and $x_2$ are both 0, the only value affecting $z$ is $b$. Because we want the result for (0, 0) to be close to zero, $b$ should be negative (at least -10)\n",
    "* If either $x_1$ or $x_2$ is 1, we want the output to be close to 1. That means the weights associated with $x_1$ and $x_2$ should be enough to offset $b$ to the point of causing $z$ to be at least 10.\n",
    "* Let's give $b$ a value of -10. How big do we need $w_1$ and $w_2$ to be? \n",
    "    * At least +20\n",
    "* So let's try out $w_1=20$, $w_2=20$, and $b=-10$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "proper-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logic_gate(w1, w2, b):\n",
    "    # Helper to create logic gate functions\n",
    "    # Plug in values for weight_a, weight_b, and bias\n",
    "    return lambda x1, x2: sigmoid(w1 * x1 + w2 * x2 + b)\n",
    "\n",
    "def test(gate):\n",
    "    # Helper function to test out our weight functions.\n",
    "    for a, b in (0, 0), (0, 1), (1, 0), (1, 1):\n",
    "        print(\"{}, {}: {}\".format(a, b, np.round(gate(a, b))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "marine-mandate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0: 0.0\n",
      "0, 1: 1.0\n",
      "1, 0: 1.0\n",
      "1, 1: 1.0\n"
     ]
    }
   ],
   "source": [
    "or_gate = logic_gate(20, 20, -10)\n",
    "test(or_gate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-truth",
   "metadata": {},
   "source": [
    "<table>\n",
    "\n",
    "<tr>\n",
    "<th colspan=\"3\">OR gate truth table</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th colspan=\"2\">Input</th>\n",
    "<th>Output</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "Try finding the appropriate weight values for each truth table. \n",
    "\n",
    "### AND Gate\n",
    "\n",
    "<table>\n",
    "\n",
    "<tr>\n",
    "<th colspan=\"3\">AND gate truth table</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th colspan=\"2\">Input</th>\n",
    "<th>Output</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-catering",
   "metadata": {},
   "source": [
    "\n",
    "Try to figure out what values for the neurons would make this function as an AND gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "composed-quilt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0: 0.0\n",
      "0, 1: 0.0\n",
      "1, 0: 0.0\n",
      "1, 1: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Fill in the w1, w2, and b parameters such that the truth table matches\n",
    "w1 = 10\n",
    "w2 = 10\n",
    "b = -10\n",
    "and_gate = logic_gate(w1, w2, b)\n",
    "\n",
    "test(and_gate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-cherry",
   "metadata": {},
   "source": [
    "Do the same for the NOR gate and the NAND gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0845cfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0: 1.0\n",
      "0, 1: 1.0\n",
      "1, 0: 1.0\n",
      "1, 1: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Fill in the w1, w2, and b parameters such that the truth table matches\n",
    "w1 = -10\n",
    "w2 = -10\n",
    "b = 20\n",
    "nand_gate = logic_gate(w1, w2, b)\n",
    "\n",
    "test(nand_gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c4fcd4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0: 1.0\n",
      "0, 1: 0.0\n",
      "1, 0: 0.0\n",
      "1, 1: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Fill in the w1, w2, and b parameters such that the truth table matches\n",
    "w1 = -10\n",
    "w2 = -10\n",
    "b = 10\n",
    "nor_gate = logic_gate(w1, w2, b)\n",
    "\n",
    "test(nor_gate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-vacation",
   "metadata": {},
   "source": [
    "## Limitation of single neuron\n",
    "\n",
    " Here's the truth table for XOR:\n",
    "\n",
    "### XOR (Exclusive Or) Gate\n",
    "\n",
    "<table>\n",
    "\n",
    "<tr>\n",
    "<th colspan=\"3\">XOR gate truth table</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<th colspan=\"2\">Input</th>\n",
    "<th>Output</th>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "Now the question is, can you create a set of weights such that a single neuron can output this property?\n",
    "\n",
    "It turns out that you cannot. Single neurons can't correlate inputs, so it's just confused. So individual neurons are out. Can we still use neurons to somehow form an XOR gate?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f0568c",
   "metadata": {},
   "source": [
    "Yes, we can somehow create a set of weight that it can output the XOR gate if you round them off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "individual-teach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0: 0.0\n",
      "0, 1: 1.0\n",
      "1, 0: 1.0\n",
      "1, 1: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Make sure you have or_gate, nand_gate, and and_gate working from above!\n",
    "def xor_gate(a, b):\n",
    "    c = np.round(or_gate(a, b))\n",
    "    d = np.round(nand_gate(a, b))\n",
    "    return and_gate(c, d)\n",
    "    \n",
    "test(xor_gate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-giant",
   "metadata": {},
   "source": [
    "## Feedforward Networks \n",
    "\n",
    "The feed-forward computation of a neural network can be thought of as matrix calculations and activation functions.  We will do some actual computations with matrices to see this in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-investigator",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Provided below are the following:\n",
    "\n",
    "- Three weight matrices `W_1`, `W_2` and `W_3` representing the weights in each layer.  The convention for these matrices is that each $W_{i,j}$ gives the weight from neuron $i$ in the previous (left) layer to neuron $j$ in the next (right) layer.  \n",
    "- A vector `x_in` representing a single input and a matrix `x_mat_in` representing 7 different inputs.\n",
    "- Two functions: `soft_max_vec` and `soft_max_mat` which apply the soft_max function to a single vector, and row-wise to a matrix.\n",
    "\n",
    "The goals for this exercise are:\n",
    "1. For input `x_in` calculate the inputs and outputs to each layer (assuming sigmoid activations for the middle two layers and soft_max output for the final layer.\n",
    "2. Write a function that does the entire neural network calculation for a single input\n",
    "3. Write a function that does the entire neural network calculation for a matrix of inputs, where each row is a single input.\n",
    "4. Test your functions on `x_in` and `x_mat_in`.\n",
    "\n",
    "This illustrates what happens in a NN during one single forward pass. Roughly speaking, after this forward pass, it remains to compare the output of the network to the known truth values, compute the gradient of the loss function and adjust the weight matrices `W_1`, `W_2` and `W_3` accordingly, and iterate. Hopefully this process will result in better weight matrices and our loss will be smaller afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "acting-mills",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the matrix W_1\n",
      "\n",
      "[[ 2 -1  1  4]\n",
      " [-1  2 -3  1]\n",
      " [ 3 -2 -1  5]]\n",
      "------------------------------\n",
      "vector input x_in\n",
      "\n",
      "[0.5 0.8 0.2]\n",
      "------------------------------\n",
      "matrix input x_mat_in -- starts with the vector `x_in`\n",
      "\n",
      "[[0.5 0.8 0.2]\n",
      " [0.1 0.9 0.6]\n",
      " [0.2 0.2 0.3]\n",
      " [0.6 0.1 0.9]\n",
      " [0.5 0.5 0.4]\n",
      " [0.9 0.1 0.9]\n",
      " [0.1 0.8 0.7]]\n"
     ]
    }
   ],
   "source": [
    "W_1 = np.array([[2,-1,1,4],[-1,2,-3,1],[3,-2,-1,5]])\n",
    "W_2 = np.array([[3,1,-2,1],[-2,4,1,-4],[-1,-3,2,-5],[3,1,1,1]])\n",
    "W_3 = np.array([[-1,3,-2],[1,-1,-3],[3,-2,2],[1,2,1]])\n",
    "x_in = np.array([.5,.8,.2])\n",
    "x_mat_in = np.array([[.5,.8,.2],[.1,.9,.6],[.2,.2,.3],[.6,.1,.9],[.5,.5,.4],[.9,.1,.9],[.1,.8,.7]])\n",
    "\n",
    "def soft_max_vec(vec):\n",
    "    return np.exp(vec)/(np.sum(np.exp(vec)))\n",
    "\n",
    "def soft_max_mat(mat):\n",
    "    return np.exp(mat)/(np.sum(np.exp(mat),axis=1).reshape(-1,1))\n",
    "\n",
    "print('the matrix W_1\\n')\n",
    "print(W_1)\n",
    "print('-'*30)\n",
    "print('vector input x_in\\n')\n",
    "print(x_in)\n",
    "print ('-'*30)\n",
    "print('matrix input x_mat_in -- starts with the vector `x_in`\\n')\n",
    "print(x_mat_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-moisture",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "1. Get the product of array x_in and W_1 (z2)\n",
    "2. Apply sigmoid function to z2 that results to a2\n",
    "3. Get the product of a2 and z2 (z3)\n",
    "4. Apply sigmoid function to z3 that results to a3\n",
    "5. Get the product of a3 and z3 that results to z4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "compact-formation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8  0.7 -2.1  3.8]\n"
     ]
    }
   ],
   "source": [
    "#1. Get the product of array x_in and W_1 (z2)\n",
    "def product_layer(W, x):\n",
    "    return np.dot(W, x)\n",
    "z2 = product_layer(W_1.T, x_in)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "32dc8439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sigmoid function to z2 that results to a2\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "vals = z2\n",
    "a2 = sigmoid(vals)\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "fig.suptitle('Sigmoid function for a2')\n",
    "plt.plot(vals, a2)\n",
    "plt.grid(True, which='both')\n",
    "plt.axhline(y=0, color='k')\n",
    "plt.axvline(x=0, color='k')\n",
    "plt.yticks()\n",
    "plt.ylim([-0.5, 1.5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d03dbef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68997448 0.66818777 0.10909682 0.97811873]\n"
     ]
    }
   ],
   "source": [
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "77c22e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8,  0.7, -2.1,  3.8])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e4914db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.507458871351723\n"
     ]
    }
   ],
   "source": [
    "z3 = np.dot(a2, z2)\n",
    "print(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "23cfcd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sigmoid function to z2 that results to a2\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "vals = z3\n",
    "a3 = sigmoid(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "abe2e05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.458299678635824\n"
     ]
    }
   ],
   "source": [
    "z4 = product_layer(a3,z3)\n",
    "print(z4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "distributed-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_max_vec(vec):\n",
    "    return np.exp(vec)/(np.sum(np.exp(vec)))\n",
    "\n",
    "def soft_max_mat(mat):\n",
    "    return np.exp(mat)/(np.sum(np.exp(mat),axis=1).reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-belle",
   "metadata": {},
   "source": [
    "7. Apply soft_max_vec function to z4 that results to y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "conscious-simulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#type your code here\n",
    "soft_max_z4 = soft_max_vec(z4)\n",
    "print(soft_max_z4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "institutional-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A one-line function to do the entire neural net computation\n",
    "\n",
    "def nn_comp_vec(x):\n",
    "    return soft_max_vec(sigmoid(sigmoid(np.dot(x,W_1)).dot(W_2)).dot(W_3))\n",
    "\n",
    "def nn_comp_mat(x):\n",
    "    return soft_max_mat(sigmoid(sigmoid(np.dot(x,W_1)).dot(W_2)).dot(W_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "authorized-patio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72780576, 0.26927918, 0.00291506])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_comp_vec(x_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "recreational-agenda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.72780576, 0.26927918, 0.00291506],\n",
       "       [0.62054212, 0.37682531, 0.00263257],\n",
       "       [0.69267581, 0.30361576, 0.00370844],\n",
       "       [0.36618794, 0.63016955, 0.00364252],\n",
       "       [0.57199769, 0.4251982 , 0.00280411],\n",
       "       [0.38373781, 0.61163804, 0.00462415],\n",
       "       [0.52510443, 0.4725011 , 0.00239447]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_comp_mat(x_mat_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-offering",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "The backpropagation in this part will be used to train a multi-layer perceptron (with a single hidden layer).  Different patterns will be used and the demonstration on how the weights will converge. The different parameters such as learning rate, number of iterations, and number of data points will be demonstrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "working-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminaries\n",
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-nothing",
   "metadata": {},
   "source": [
    "Fill out the code below so that it creates a multi-layer perceptron with a single hidden layer (with 4 nodes) and trains it via back-propagation.  Specifically your code should:\n",
    "\n",
    "1. Initialize the weights to random values between -1 and 1\n",
    "1. Perform the feed-forward computation\n",
    "1. Compute the loss function\n",
    "1. Calculate the gradients for all the weights via back-propagation\n",
    "1. Update the weight matrices (using a learning_rate parameter)\n",
    "1. Execute steps 2-5 for a fixed number of iterations\n",
    "1. Plot the accuracies and log loss and observe how they change over time\n",
    "\n",
    "\n",
    "Once your code is running, try it for the different patterns below.\n",
    "\n",
    "- Which patterns was the neural network able to learn quickly and which took longer?\n",
    "- What learning rates and numbers of iterations worked well?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525aca0a",
   "metadata": {},
   "source": [
    "The neural network learned simpler patterns such as the diamond and centered square more quickly, while patterns with sharper or more complex boundaries took longer to converge. Moderate learning rates (around 0.01â€“0.03) worked best, with fewer iterations needed for simple patterns and more iterations required for complex ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "27490ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_2 = np.random.uniform(-1, 1, size=(4, 1))\n",
    "y_pred, _ = forward_pass(W_1, W_2)\n",
    "y_pred = y_pred.reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "greatest-crown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_mat_full is (500, 3)\n",
      "shape of y is (500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jnrss\\AppData\\Local\\Temp\\ipykernel_9176\\309342056.py:32: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"ro\" (-> color='r'). The keyword argument will take precedence.\n",
      "  ax.plot(x_mat_full[y==1, 0],x_mat_full[y==1, 1], 'ro', label='class 1', color='darkslateblue')\n",
      "C:\\Users\\jnrss\\AppData\\Local\\Temp\\ipykernel_9176\\309342056.py:33: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"bx\" (-> color='b'). The keyword argument will take precedence.\n",
      "  ax.plot(x_mat_full[y==0, 0],x_mat_full[y==0, 1], 'bx', label='class 0', color='chocolate')\n"
     ]
    }
   ],
   "source": [
    "## This code below generates two x values and a y value according to different patterns\n",
    "## It also creates a \"bias\" term (a vector of 1s)\n",
    "## The goal is then to learn the mapping from x to y using a neural network via back-propagation\n",
    "\n",
    "num_obs = 500\n",
    "x_mat_1 = np.random.uniform(-1,1,size = (num_obs,2))\n",
    "x_mat_bias = np.ones((num_obs,1))\n",
    "x_mat_full = np.concatenate( (x_mat_1,x_mat_bias), axis=1)\n",
    "\n",
    "# PICK ONE PATTERN BELOW and comment out the rest.\n",
    "\n",
    "# # Circle pattern\n",
    "# y = (np.sqrt(x_mat_full[:,0]**2 + x_mat_full[:,1]**2)<.75).astype(int)\n",
    "\n",
    "# # Diamond Pattern\n",
    "y = ((np.abs(x_mat_full[:,0]) + np.abs(x_mat_full[:,1]))<1).astype(int)\n",
    "\n",
    "# # Centered square\n",
    "# y = ((np.maximum(np.abs(x_mat_full[:,0]), np.abs(x_mat_full[:,1])))<.5).astype(int)\n",
    "\n",
    "# # Thick Right Angle pattern\n",
    "# y = (((np.maximum((x_mat_full[:,0]), (x_mat_full[:,1])))<.5) & ((np.maximum((x_mat_full[:,0]), (x_mat_full[:,1])))>-.5)).astype(int)\n",
    "\n",
    "# # Thin right angle pattern\n",
    "# y = (((np.maximum((x_mat_full[:,0]), (x_mat_full[:,1])))<.5) & ((np.maximum((x_mat_full[:,0]), (x_mat_full[:,1])))>0)).astype(int)\n",
    "\n",
    "\n",
    "print('shape of x_mat_full is {}'.format(x_mat_full.shape))\n",
    "print('shape of y is {}'.format(y.shape))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(x_mat_full[y==1, 0],x_mat_full[y==1, 1], 'ro', label='class 1', color='darkslateblue')\n",
    "ax.plot(x_mat_full[y==0, 0],x_mat_full[y==0, 1], 'bx', label='class 0', color='chocolate')\n",
    "# ax.grid(True)\n",
    "ax.legend(loc='best')\n",
    "ax.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "closing-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid function\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred, eps=1e-16):\n",
    "    \"\"\"\n",
    "    Loss function we would like to optimize (minimize)\n",
    "    We are using Logarithmic Loss\n",
    "    http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss\n",
    "    \"\"\"\n",
    "    y_pred = np.maximum(y_pred,eps)\n",
    "    y_pred = np.minimum(y_pred,(1-eps))\n",
    "    return -(np.sum(y_true * np.log(y_pred)) + np.sum((1-y_true)*np.log(1-y_pred)))/len(y_true)\n",
    "\n",
    "\n",
    "def forward_pass(W1, W2):\n",
    "    \"\"\"\n",
    "    Does a forward computation of the neural network\n",
    "    Takes the input `x_mat` (global variable) and produces the output `y_pred`\n",
    "    Also produces the gradient of the log loss function\n",
    "    \"\"\"\n",
    "    global x_mat\n",
    "    global y\n",
    "    global num_\n",
    "    # First, compute the new predictions `y_pred`\n",
    "    z_2 = np.dot(x_mat, W_1)\n",
    "    a_2 = sigmoid(z_2)\n",
    "    z_3 = np.dot(a_2, W_2)\n",
    "    y_pred = sigmoid(z_3).reshape((len(x_mat),))\n",
    "    # Now compute the gradient\n",
    "    J_z_3_grad = -y + y_pred\n",
    "    J_W_2_grad = np.dot(J_z_3_grad, a_2)\n",
    "    a_2_z_2_grad = sigmoid(z_2)*(1-sigmoid(z_2))\n",
    "    J_W_1_grad = (np.dot((J_z_3_grad).reshape(-1,1), W_2.reshape(-1,1).T)*a_2_z_2_grad).T.dot(x_mat).T\n",
    "    gradient = (J_W_1_grad, J_W_2_grad)\n",
    "    \n",
    "    # return\n",
    "    return y_pred, gradient\n",
    "\n",
    "\n",
    "def plot_loss_accuracy(loss_vals, accuracies):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    fig.suptitle('Log Loss and Accuracy over iterations')\n",
    "    \n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(loss_vals)\n",
    "    ax.grid(True)\n",
    "    ax.set(xlabel='iterations', title='Log Loss')\n",
    "    \n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.plot(accuracies)\n",
    "    ax.grid(True)\n",
    "    ax.set(xlabel='iterations', title='Accuracy');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-union",
   "metadata": {},
   "source": [
    "Complete the pseudocode below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "elect-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Initialize the network parameters\n",
    "\n",
    "np.random.seed(1241)\n",
    "\n",
    "W_1 = W_1\n",
    "W_2 = W_2\n",
    "num_iter = 1000\n",
    "learning_rate = 0.02\n",
    "x_mat = x_mat_full\n",
    "\n",
    "\n",
    "\n",
    "loss_vals, accuracies = [], []\n",
    "for i in range(num_iter):\n",
    "    ### Do a forward computation, and get the gradient\n",
    "    \n",
    "    ## Update the weight matrices\n",
    "    \n",
    "    ### Compute the loss and accuracy\n",
    "\n",
    "    ## Print the loss and accuracy for every 200th iteration\n",
    "    \n",
    "    plot_loss_accuracy(loss_vals, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-consideration",
   "metadata": {},
   "source": [
    "Plot the predicted answers, with mistakes in yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "willing-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(W_1, W_2):\n",
    "    global x_mat, y\n",
    "\n",
    "    z_2 = np.dot(x_mat, W_1)          # (N,4)\n",
    "    a_2 = sigmoid(z_2)                # (N,4)\n",
    "\n",
    "    # ensure W_2 is (4,1) so z_3 becomes (N,1)\n",
    "    if W_2.ndim == 1:\n",
    "        W_2 = W_2.reshape(-1, 1)\n",
    "    if W_2.shape != (4, 1):\n",
    "        W_2 = W_2.reshape(4, 1)\n",
    "\n",
    "    z_3 = np.dot(a_2, W_2)            # (N,1)\n",
    "    y_pred = sigmoid(z_3).reshape(-1) # (N,)\n",
    "\n",
    "    # gradients (keep yours, just fix shapes)\n",
    "    J_z_3_grad = (-y + y_pred)                    # (N,)\n",
    "    J_W_2_grad = np.dot(a_2.T, J_z_3_grad.reshape(-1,1))  # (4,1)\n",
    "\n",
    "    a_2_z_2_grad = a_2 * (1 - a_2)                # (N,4)\n",
    "    J_W_1_grad = x_mat.T.dot((J_z_3_grad.reshape(-1,1) @ W_2.T) * a_2_z_2_grad)  # (3,4)\n",
    "\n",
    "    gradient = (J_W_1_grad, J_W_2_grad)\n",
    "    return y_pred, gradient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-guinea",
   "metadata": {},
   "source": [
    "#### Supplementary Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-breakdown",
   "metadata": {},
   "source": [
    "1. Use a different weights , input and activation function\n",
    "2. Apply feedforward and backpropagation\n",
    "3. Plot the loss and accuracy for every 300th iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "755c0822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.6940626351185337 acc: 0.434\n",
      "300 loss: 0.18733768148453736 acc: 0.916\n",
      "600 loss: 0.17749285865253236 acc: 0.914\n",
      "900 loss: 0.17434896355687649 acc: 0.918\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2026)\n",
    "\n",
    "x_mat = x_mat_full\n",
    "# y is already created from the pattern cell\n",
    "\n",
    "W_1 = np.random.uniform(-1, 1, size=(3, 4))\n",
    "W_2 = np.random.uniform(-1, 1, size=(4, 1))\n",
    "\n",
    "num_iter = 1200\n",
    "learning_rate = 0.02\n",
    "\n",
    "loss_vals, accuracies = [], []\n",
    "\n",
    "for i in range(num_iter):\n",
    "    y_pred, grads = forward_pass(W_1, W_2)\n",
    "    dW1, dW2 = grads\n",
    "\n",
    "    # update\n",
    "    W_1 -= learning_rate * dW1\n",
    "    W_2 -= learning_rate * dW2\n",
    "\n",
    "    # metrics\n",
    "    loss_vals.append(loss_fn(y, y_pred))\n",
    "    accuracies.append(np.mean((y_pred >= 0.5) == (y == 1)))\n",
    "\n",
    "    if i % 300 == 0:\n",
    "        print(i, \"loss:\", loss_vals[-1], \"acc:\", accuracies[-1])\n",
    "\n",
    "plot_loss_accuracy(loss_vals, accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-excess",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-segment",
   "metadata": {},
   "source": [
    "This activity successfully met the intended learning outcomes. The use of sigmoid and ReLU functions demonstrated how activation functions introduce non-linearity in neural networks and influence decision boundaries. Additionally, the implementation of feedforward computation and backpropagation through gradient descent showed how neural networks learn from data by updating weights to minimize loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eb2746",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
